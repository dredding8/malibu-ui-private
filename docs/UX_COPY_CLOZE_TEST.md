# UX Copy Understandability: Cloze Test

## What is a Cloze Test?

A cloze test removes key words from text and asks participants to fill in the blanks. It measures comprehension and whether copy uses language that matches users' mental models.

**Test Duration:** 10-15 minutes
**Participants Needed:** 15-20 users (mix of novice and experienced)
**Format:** Remote, unmoderated survey

---

## Instructions for Participants

> Read each passage below. Some words have been removed and replaced with blanks.
>
> **Fill in each blank with the word you think best fits.**
>
> - Use your first instinctâ€”don't overthink it
> - If you're unsure, write your best guess
> - You can write "I don't know" if you're stuck
>
> This helps us understand if our application uses clear, predictable language.

---

## Test Passages

### Passage 1: Collection Opportunities Table Headers

> You're viewing a table of satellite collection opportunities. The column headers are:
>
> - _______ â€” Shows if the opportunity is active, pending, or complete
> - _______ â€” The name of the collection pass
> - _______ â€” Which satellite will be used
> - _______ â€” How much collection capacity is available
> - _______ â€” Importance level (P1-P4)
> - _______ â€” What you can do with this opportunity

**Expected answers:**
- Blank 1: Status
- Blank 2: Opportunity / Name
- Blank 3: Satellite
- Blank 4: Capacity
- Blank 5: Priority
- Blank 6: Actions

**What we're testing:** Are table column headers clear and predictable?

---

### Passage 2: Unified Editor - Quick Edit Mode

> You open an editor to make a fast change to a collection opportunity. The form shows:
>
> "_______ : WORLDVIEW-3 North Pass"
>
> Below that, you can change the _______ level (P1-P4).
>
> Your current site assignments are shown as _______ -only (you need Standard Edit to change sites).

**Expected answers:**
- Blank 1: Editing / Edit / Modifying
- Blank 2: Priority / Importance
- Blank 3: read / view / display

**What we're testing:** Is "Editing:" the right header? Is priority terminology clear? Do users understand "read-only"?

---

### Passage 3: Unified Editor - Override Mode Warning

> When you enter Override Mode, you see a yellow warning banner:
>
> "_______ _______ Mode: Changes made here override system recommendations. Ensure you provide detailed _______ for audit compliance."

**Expected answers:**
- Blank 1-2: Manual Override
- Blank 3: justification / explanation / reason

**What we're testing:** Is "Manual Override" clear? Do users understand they need justification?

---

### Passage 4: Allocation Tab - Site Selection

> In the Override Editor, you're on the "_______ " tab. This is where you assign collection opportunities to ground stations.
>
> A section header says "_______ Sites" showing the sites you've selected.
>
> You can adjust how many _______ each site will handle.

**Expected answers:**
- Blank 1: Allocation / Assignment
- Blank 2: Allocated / Selected / Assigned
- Blank 3: collects / collections / passes

**What we're testing:** Is "Allocation" understandable? Is "collects" a clear term?

---

### Passage 5: Override Workflow Tab Labels

> The Override Editor has three tabs:
>
> "1. _______" â€” Choose which sites to use
> "2. _______" â€” Explain why you're overriding the system
> "3. _______" â€” Check everything before saving

**Expected answers:**
- Blank 1: Allocation / Sites / Assignment
- Blank 2: Justification / Explanation / Reason
- Blank 3: Review / Summary / Confirm

**What we're testing:** Are the tab names obvious for a 3-step workflow?

---

### Passage 6: Dashboard Actions

> You need to update your satellite information. On the Dashboard, you see three buttons:
>
> - "_______ Data Sources" â€” Gets the latest satellite data
> - "Create _______" â€” Starts building a new collection schedule
> - "Add Data _______" â€” Manually enters a new satellite

**Expected answers:**
- Blank 1: Refresh / Update / Reload
- Blank 2: Collection / Deck
- Blank 3: Source

**What we're testing:** Do users understand "Refresh" vs "Update"? Do they know what a "Collection" is?

---

### Passage 7: Collection Creation Wizard

> You're creating a collection. The progress bar shows you're on Step 2 of 4.
>
> A message appears: "_______ Saved: We've saved your work automatically. You can leave and come back anytime."
>
> This means your data is being saved in the _______ (cloud/background/draft) so you won't _______ your work if you close the browser.

**Expected answers:**
- Blank 1: Progress / Work / Changes
- Blank 2: background / draft / cloud
- Blank 3: lose

**What we're testing:** Do users understand autosave messaging? Is "Progress Saved" the right phrase?

---

### Passage 8: Collection Opportunities

> You're viewing satellite passes you can schedule. You see 15 opportunities listed.
>
> A checkbox says: "Show all _______ _______"
>
> Currently it's unchecked, so you're only seeing _______ quality passes. If you check it, you'll see _______ quality passes too.

**Expected answers:**
- Blank 1-2: quality tiers / quality levels / pass types
- Blank 3: optimal / best / high / good
- Blank 4: lower / suboptimal / baseline / all

**What we're testing:** Is "quality tiers" understandable? Do users know what's hidden?

---

### Passage 9: Background Processing

> After clicking "Submit," you see this message:
>
> "We're _______ on Your Collection: Your collection is being built in the _______."
>
> This means you _______ (can/must) navigate away from this page while it processes.

**Expected answers:**
- Blank 1: Working / Processing
- Blank 2: background / cloud / system
- Blank 3: can

**What we're testing:** Does "working" convey active processing? Is "background" clear?

---

### Passage 10: Error Recovery

> You tried to refresh data sources but it failed. You see:
>
> "Refresh _______: Network error: Unable to connect to server"
>
> Below is a button labeled "_______ _______"
>
> Clicking this button will _______ the refresh operation.

**Expected answers:**
- Blank 1: failed / error / unsuccessful
- Blank 2-3: Try Again / Retry / Refresh Again
- Blank 4: retry / attempt / repeat

**What we're testing:** Is "Try Again" clear for error recovery?

---

### Passage 11: Data Source Information

> You're adding a new satellite to the system. The form is titled "_______ Information."
>
> SCC stands for "_______  _______  _______" or "Individual satellite or collection system."
>
> You need to enter the satellite's _______ number, priority level, and orbit type.

**Expected answers:**
- Blank 1: SCC / Satellite / Source
- Blank 2-4: Space / Satellite / Collection (or "I don't know")
- Blank 5: SCC / identification / ID

**What we're testing:** Do users understand "SCC"? Should we spell it out?

---

### Passage 12: Wizard Progress Context

> You're in Step 2 of the collection wizard. You see:
>
> "Configure your collection _______ and site requirements"
>
> "Estimated time remaining: 5-7 _______"
>
> This time estimate refers to how long it will take to complete the _______ steps (not this step alone).

**Expected answers:**
- Blank 1: parameters / settings / options
- Blank 2: minutes / min
- Blank 3: remaining / next / future

**What we're testing:** Is "parameters" too technical? Is time estimate clear?

---

### Passage 13: Navigation Breadcrumbs

> At the top of the page, you see:
>
> Data Sources > SCCs > _______ SCC
>
> These are called _______. They show your location in the app.
>
> If you click "Data Sources," you'll go back to the _______ page.

**Expected answers:**
- Blank 1: Add / New
- Blank 2: breadcrumbs / navigation / links
- Blank 3: Dashboard / home / main / Data Sources

**What we're testing:** Do users recognize breadcrumb navigation?

---

### Passage 14: Status Messages

> While data is loading, you see:
>
> "_______ master list... Please wait while we _______ with the server. This may take a few _______."
>
> You should _______ (wait/click around/refresh) until it finishes.

**Expected answers:**
- Blank 1: Updating / Loading / Refreshing
- Blank 2: synchronize / sync / connect / communicate
- Blank 3: moments / seconds / minutes
- Blank 4: wait

**What we're testing:** Is "synchronize" too technical? Does "moments" convey time?

---

### Passage 15: Abandonment Warning

> You click "Cancel" while creating a collection. An alert appears:
>
> "You've made _______ on your collection. Are you sure you want to start over?"
>
> "Note: We can't _______ this work once it's gone. All your entered information will be _______."
>
> Two buttons appear: "Continue _______" and "_______ Changes"

**Expected answers:**
- Blank 1: progress / changes / work
- Blank 2: restore / recover / retrieve
- Blank 3: lost / deleted / removed
- Blank 4: Editing / Working
- Blank 5: Discard / Delete / Remove

**What we're testing:** Is the warning clear? Are button labels obvious?

---

## Scoring Guidelines

### For Each Blank:

**Exact Match (3 points)**
- User wrote the exact word we use in the app
- Example: Blank says "Refresh" and user wrote "Refresh"

**Synonym Match (2 points)**
- User wrote a reasonable synonym that shows comprehension
- Example: Blank is "Refresh" and user wrote "Update" or "Reload"

**Conceptual Match (1 point)**
- User wrote something in the right direction but not ideal
- Example: Blank is "parameters" and user wrote "settings" or "options"

**No Match (0 points)**
- User wrote something unrelated or "I don't know"
- Example: Blank is "Refresh" and user wrote "Settings"

### Comprehension Thresholds:

**Per Passage (max 9-15 points depending on blanks):**
- **High comprehension:** â‰¥80% of max points
- **Moderate comprehension:** 60-79% of max points
- **Low comprehension:** <60% of max points (REVISE COPY)

**Per Word/Phrase:**
- **Clear terminology:** â‰¥70% of users write exact or synonym match
- **Moderately clear:** 50-69% exact/synonym match
- **Unclear/jargon:** <50% exact/synonym match (NEEDS REVISION)

---

## Analysis: What to Look For

### Red Flags:

1. **Wide variation in answers**
   - If users write completely different words, the original term is ambiguous
   - Example: Some write "Refresh," others write "Sync," others write "Get"

2. **"I don't know" frequency**
   - If >30% write "I don't know," the term is probably jargon or unclear context

3. **Wrong conceptual category**
   - User writes a word from a different domain
   - Example: Blank is "Collection" (schedule) but user writes "Folder" (file system)

4. **Overly technical answers from novices**
   - If novice users write very technical terms, they might be guessing based on what they think we want, not what they understand

### Good Signs:

1. **High exact match rate**
   - â‰¥70% of users write the exact word = predictable, clear terminology

2. **Consistent synonyms**
   - Users write different words but all conceptually correct
   - Shows the concept is clear even if specific word choice varies

3. **No "I don't know" responses**
   - Everyone has enough context to make a guess = good scaffolding

---

## Recommendations Based on Results

### If terminology scores <50% exact match:

**Option 1: Use the word users wrote**
- If 80% of users wrote "Update" instead of "Refresh," consider changing to "Update"

**Option 2: Add clarifying text**
- Keep technical term but add plain language
- Example: "Refresh Data Sources (Get Latest Updates)"

**Option 3: Add tooltip/help icon**
- Provide definition on hover for domain-specific terms
- Example: "Collection Deck â“˜" â†’ hover shows "Planned schedule of satellite collections"

### If entire passage scores low:

- **Rewrite for clarity:** Simplify sentence structure
- **Add context:** Users need more information to understand
- **Test alternative phrasing:** Try different words and re-test

---

## Quick Setup Instructions

### Using Google Forms:

1. Create a new form
2. Add demographic questions (experience level, role)
3. Add each passage as a separate section
4. Use "Short answer" question type for each blank
5. Make all questions required
6. Add consent question at start
7. Add "Thank you" message at end with contact info

### Using Qualtrics/SurveyMonkey:

1. Create survey with text piping for blanks
2. Randomize passage order to avoid fatigue bias
3. Set 30-minute timeout
4. Export to CSV for analysis

### Using TypeForm:

1. More engaging UI for participants
2. Shows one question at a time (less overwhelming)
3. Built-in analytics

---

## Sample Results Table

| Passage | Term | Our Word | Top User Responses | Match % | Action |
|---------|------|----------|-------------------|---------|--------|
| 1 | Table header | Opportunity | Opportunity (70%), Name (20%), Pass (10%) | 90% | âœ… Keep |
| 2 | Editor label | Editing | Editing (80%), Edit (15%), Modify (5%) | 95% | âœ… Keep |
| 3 | Warning mode | Manual Override | Manual Override (60%), Override (25%), Manual (10%) | 85% | âœ… Keep |
| 4 | Tab name | Allocation | Allocation (45%), Assignment (35%), Sites (15%) | 95% | âœ… Keep (synonyms OK) |
| 6 | Data update | Refresh | Refresh (65%), Update (30%), Reload (5%) | 95% | âœ… Keep |
| 8 | Quality level | quality tiers | levels (40%), types (30%), tiers (20%), ??? (10%) | 90% | ðŸŸ¡ Consider "quality levels" |
| 11 | SCC meaning | (acronym) | ??? (60%), Satellite (25%), System (10%) | 35% | ðŸ”´ Must spell out or define |
| 14 | Server action | synchronize | sync (50%), connect (25%), update (15%) | 90% | ðŸŸ¡ Consider "sync" |

**Legend:**
- âœ… Keep current copy (â‰¥70% match)
- ðŸŸ¡ Consider revision (50-69% match)
- ðŸ”´ Must revise (<50% match)

---

## Timeline

- **Setup:** 1 day (create survey)
- **Pilot:** 2-3 participants, 2 days (validate clarity of test itself)
- **Full test:** 15-20 participants, 1 week
- **Analysis:** 2 days (score responses, identify patterns)
- **Report:** 1 day (recommendations with examples)

**Total:** ~2 weeks

---

## Example Follow-Up Questions (Optional)

After cloze test, ask:

1. "Which passage was hardest to complete?"
   - Identifies most confusing section

2. "Were there any words you weren't familiar with?"
   - Catches jargon we missed

3. "If you could change one label/term to make it clearer, what would it be?"
   - Gets direct user suggestions

---

*Cloze Test for: Satellite Collection Management Application*
*Method: Standard cloze deletion (every Nth word removed, or targeted key terms)*
*Expected completion time: 10-15 minutes*
*Participant incentive: $15-20*
